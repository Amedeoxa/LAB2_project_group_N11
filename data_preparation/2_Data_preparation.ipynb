{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Data preparation**"
      ],
      "metadata": {
        "id": "0nn4wZc5_0AI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Generating non redundant datasets**"
      ],
      "metadata": {
        "id": "k9gHIEB3_ikk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "FbfwUM9snZ1v"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def filter_redundancy(unique_ids, total_dataset):\n",
        "  base, ext = os.path.splitext(total_dataset)\n",
        "  output_file = f\"{base}_non_redundant{ext}\"\n",
        "  with open(unique_ids, 'r') as clustered_pos, open(total_dataset, 'r') as tot_pos, open(output_file, 'w') as non_redundant_dataset:\n",
        "    unique_ids = [name.rstrip() for name in clustered_pos]\n",
        "    non_redundant_dataset.write(\n",
        "        \"\".join(\n",
        "            line.rstrip() + \"\\n\"\n",
        "            for line in tot_pos\n",
        "            if line.split('\\t')[0] in unique_ids\n",
        "        ).rstrip(\"\\n\")\n",
        "    )\n",
        "  return output_file\n",
        "\n",
        "non_redundant_pos = filter_redundancy('uniq.pos.tsv', 'positive.tsv')\n",
        "non_redundant_neg = filter_redundancy('uniq.neg.tsv', 'negative.tsv')"
      ],
      "metadata": {
        "id": "WlVyNj5rnjAM"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Dividing in train and test set**"
      ],
      "metadata": {
        "id": "24RW7E8os5qL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random"
      ],
      "metadata": {
        "id": "3CSqeFuqBOko"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def training_test_split(file):\n",
        "  with open(file, 'r') as dataset:\n",
        "    lines = [line.rstrip() for line in dataset]\n",
        "    random.seed(50)\n",
        "    random.shuffle(lines)\n",
        "    n = 80*len(lines)//100\n",
        "    training_set = lines[:n]\n",
        "    test_set = lines[n:]\n",
        "  return training_set, test_set\n",
        "\n",
        "with open('training_set.tsv', 'w') as training_set, open('test_set.tsv', 'w') as test_set:\n",
        "  training_set_n, test_set_n = training_test_split('negative_non_redundant.tsv')\n",
        "  training_set_p, test_set_p = training_test_split('positive_non_redundant.tsv')\n",
        "  training = training_set_p + training_set_n\n",
        "  test = test_set_p + test_set_n\n",
        "  for i in range(len(training)):\n",
        "    if i < len(training)-1:\n",
        "      training_set.write(training[i] + '\\n')\n",
        "    else:\n",
        "      training_set.write(training[i])\n",
        "  for i in range(len(test)):\n",
        "    if i < len(test)-1:\n",
        "      test_set.write(test[i] + '\\n')\n",
        "    else:\n",
        "      test_set.write(test[i])\n",
        ""
      ],
      "metadata": {
        "id": "9TgDZK0xBbJ1"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_validation_sets(total_training):\n",
        "    n = int((20*len(total_training))/100)\n",
        "    validation_1 = total_training[:n]\n",
        "    validation_2 = total_training[n:2*n]\n",
        "    validation_3 = total_training[2*n:3*n]\n",
        "    validation_4 = total_training[3*n:4*n]\n",
        "    validation_5 = total_training[4*n:]\n",
        "    return validation_1, validation_2, validation_3, validation_4, validation_5\n",
        "\n",
        "p_validation_1, p_validation_2, p_validation_3, p_validation_4, p_validation_5 = create_validation_sets(training_set_p)\n",
        "n_validation_1, n_validation_2, n_validation_3, n_validation_4, n_validation_5 = create_validation_sets(training_set_n)\n",
        "validation_1 = p_validation_1 + n_validation_1\n",
        "validation_2 = p_validation_2 + n_validation_2\n",
        "validation_3 = p_validation_3 + n_validation_3\n",
        "validation_4 = p_validation_4 + n_validation_4\n",
        "validation_5 = p_validation_5 + n_validation_5"
      ],
      "metadata": {
        "id": "NSR-yJZv211q"
      },
      "execution_count": 21,
      "outputs": []
    }
  ]
}